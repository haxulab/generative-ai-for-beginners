
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../19-slm/">
      
      
        <link rel="next" href="../21-meta/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.46">
    
    
      
        <title>Building with Mistral Models - 面向初学者的生成式AI课程</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.6f8fc17f.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#building-with-mistral-models" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="面向初学者的生成式AI课程" class="md-header__button md-logo" aria-label="面向初学者的生成式AI课程" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            面向初学者的生成式AI课程
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Building with Mistral Models
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="面向初学者的生成式AI课程" class="md-nav__button md-logo" aria-label="面向初学者的生成式AI课程" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    面向初学者的生成式AI课程
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../translations/cn/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    面向初学者的生成式人工智能课程 ( Version 3 )
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../00-course-setup/translations/cn/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    课程介绍和学习环境设置
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../01-introduction-to-genai/translations/cn/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    第一章 : 生成式人工智能和 LLMs 介绍
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../02-exploring-and-comparing-different-llms/translations/cn/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    第二章 : 探索和比较不同的 LLMs
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../03-using-generative-ai-responsibly/translations/cn/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    第三章 ： 负责任地使用生成式人工智能
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../04-prompt-engineering-fundamentals/translations/cn/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    第四章：提示工程基础
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../05-advanced-prompts/translations/cn/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    第五章：创建高级的提示工程技巧
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../06-text-generation-apps/translations/cn/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    第六章：创建文本生成应用
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../07-building-chat-applications/translations/cn/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    第七章：创建聊天应用
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../08-building-search-applications/translations/cn/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    第八章：创建搜索应用
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../09-building-image-applications/translations/cn/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    第九章：构建图像生成应用
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../10-building-low-code-ai-applications/translations/cn/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    第十章：创建低代码的人工智能应用
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../11-integrating-with-function-calling/translations/cn/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    第十一章：为生成式 AI 添加 function calling
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../12-designing-ux-for-ai-applications/translations/cn/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    第十二章：为人工智能应用程序添加用户体验
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../13-securing-ai-applications/translations/cn/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    生成式 AI 初学者指南：第 13 章 - 保护 AI 应用
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../14-the-generative-ai-application-lifecycle/translations/cn/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Index
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../15-rag-and-vector-databases/translations/cn/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    检索增强生成 (RAG) 与向量数据库
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../16-open-source-models/translations/tw/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Index
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../17-ai-agents/translations/tw/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Index
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../18-fine-tuning/translations/tw/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Index
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../19-slm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction to Small Language Models for Generative AI for Beginners
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Building with Mistral Models
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Building with Mistral Models
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    <span class="md-ellipsis">
      Introduction
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-mistral-models" class="md-nav__link">
    <span class="md-ellipsis">
      The Mistral Models
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mistral-large-2-2407" class="md-nav__link">
    <span class="md-ellipsis">
      Mistral Large 2 (2407)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Mistral Large 2 (2407)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#rag-example-using-mistral-large-2" class="md-nav__link">
    <span class="md-ellipsis">
      RAG Example using Mistral Large 2
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mistral-small" class="md-nav__link">
    <span class="md-ellipsis">
      Mistral Small
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#comparing-mistral-small-and-mistral-large" class="md-nav__link">
    <span class="md-ellipsis">
      Comparing Mistral Small and Mistral Large
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mistral-nemo" class="md-nav__link">
    <span class="md-ellipsis">
      Mistral NeMo
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Mistral NeMo">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#comparing-tokenizers" class="md-nav__link">
    <span class="md-ellipsis">
      Comparing Tokenizers
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#learning-does-not-stop-here-continue-the-journey" class="md-nav__link">
    <span class="md-ellipsis">
      Learning does not stop here, continue the Journey
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../21-meta/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Building With the Meta Family Models
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    <span class="md-ellipsis">
      Introduction
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-mistral-models" class="md-nav__link">
    <span class="md-ellipsis">
      The Mistral Models
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mistral-large-2-2407" class="md-nav__link">
    <span class="md-ellipsis">
      Mistral Large 2 (2407)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Mistral Large 2 (2407)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#rag-example-using-mistral-large-2" class="md-nav__link">
    <span class="md-ellipsis">
      RAG Example using Mistral Large 2
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mistral-small" class="md-nav__link">
    <span class="md-ellipsis">
      Mistral Small
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#comparing-mistral-small-and-mistral-large" class="md-nav__link">
    <span class="md-ellipsis">
      Comparing Mistral Small and Mistral Large
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mistral-nemo" class="md-nav__link">
    <span class="md-ellipsis">
      Mistral NeMo
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Mistral NeMo">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#comparing-tokenizers" class="md-nav__link">
    <span class="md-ellipsis">
      Comparing Tokenizers
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#learning-does-not-stop-here-continue-the-journey" class="md-nav__link">
    <span class="md-ellipsis">
      Learning does not stop here, continue the Journey
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="building-with-mistral-models">Building with Mistral Models</h1>
<h2 id="introduction">Introduction</h2>
<p>This lesson will cover: 
- Exploring the different Mistral Models 
- Understanding the use-cases and scenarios for each model 
- Code samples show the unique features of each model. </p>
<h2 id="the-mistral-models">The Mistral Models</h2>
<p>In this lesson, we will explore 3 different Mistral models: 
<strong>Mistral Large</strong>, <strong>Mistral Small</strong> and <strong>Mistral Nemo</strong>. </p>
<p>Each of these models are available free on the Github Model marketplace. The code in this notebook will be using this models to run the code. Here are more details on using Github Models to <a href="https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst">prototype with AI models</a>. </p>
<h2 id="mistral-large-2-2407">Mistral Large 2 (2407)</h2>
<p>Mistral Large 2 is currently the flagship model from Mistral and is designed for enterprise use. </p>
<p>The model is an  upgrade to the original Mistral Large by offering 
-  Larger Context Window - 128k vs 32k 
-  Better performance on Math and Coding Tasks - 76.9% average accuracy vs 60.4% 
-  Increased multilingual performance - languages include: English, French, German, Spanish, Italian, Portuguese, Dutch, Russian, Chinese, Japanese, Korean, Arabic, and Hindi.</p>
<p>With these features, Mistral Large excels at 
- <em>Retrieval Augmented Generation (RAG)</em> - due to the larger context window
- <em>Function Calling</em> - this model has native function calling which allows integration with external tools and APIs. These calls can be made both in parallel or one after another in a sequential order. 
- <em>Code Generation</em> - this model excels on Python, Java, TypeScript and C++ generation. </p>
<h3 id="rag-example-using-mistral-large-2">RAG Example using Mistral Large 2</h3>
<p>In this example, we are using Mistral Large 2 to run a RAG pattern over a text document. The question is written in Korean and asks about the author's activities before college. </p>
<p>It uses Cohere Embeddings Model to create embeddings of the text document as well as the question. For this sample, it uses the faiss Python package as a vector store. </p>
<p>The prompt sent to the Mistral model includes both the questions and the retrieved chunks that are similar to the question. The Model then provides a natural language response. </p>
<pre><code class="language-python">pip install faiss-cpu
</code></pre>
<pre><code class="language-python">import requests
import numpy as np
import faiss
import os

from azure.ai.inference import ChatCompletionsClient
from azure.ai.inference.models import SystemMessage, UserMessage
from azure.core.credentials import AzureKeyCredential
from azure.ai.inference import EmbeddingsClient

endpoint = &quot;https://models.inference.ai.azure.com&quot;
model_name = &quot;Mistral-large&quot;
token = os.environ[&quot;GITHUB_TOKEN&quot;]

client = ChatCompletionsClient(
    endpoint=endpoint,
    credential=AzureKeyCredential(token),
)

response = requests.get('https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt')
text = response.text

chunk_size = 2048
chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]
len(chunks)

embed_model_name = &quot;cohere-embed-v3-multilingual&quot; 

embed_client = EmbeddingsClient(
        endpoint=endpoint,
        credential=AzureKeyCredential(token)
)

embed_response = embed_client.embed(
    input=chunks,
    model=embed_model_name
)



text_embeddings = []
for item in embed_response.data:
    length = len(item.embedding)
    text_embeddings.append(item.embedding)
text_embeddings = np.array(text_embeddings)


d = text_embeddings.shape[1]
index = faiss.IndexFlatL2(d)
index.add(text_embeddings)

question = &quot;저자가 대학에 오기 전에 주로 했던 두 가지 일은 무엇이었나요?？&quot;

question_embedding = embed_client.embed(
    input=[question],
    model=embed_model_name
)

question_embeddings = np.array(question_embedding.data[0].embedding)


D, I = index.search(question_embeddings.reshape(1, -1), k=2) # distance, index
retrieved_chunks = [chunks[i] for i in I.tolist()[0]]

prompt = f&quot;&quot;&quot;
Context information is below.
---------------------
{retrieved_chunks}
---------------------
Given the context information and not prior knowledge, answer the query.
Query: {question}
Answer:
&quot;&quot;&quot;


chat_response = client.complete(
    messages=[
        SystemMessage(content=&quot;You are a helpful assistant.&quot;),
        UserMessage(content=prompt),
    ],
    temperature=1.0,
    top_p=1.0,
    max_tokens=1000,
    model=model_name
)

print(chat_response.choices[0].message.content)
</code></pre>
<h2 id="mistral-small">Mistral Small</h2>
<p>Mistral Small is another model in the Mistral family of models under the premier/enterprise category. As the name implies, this model is a Small Language Model (SLM). The advantages of using Mistral Small are that it is: 
- Cost Saving compared to Mistral LLMs like Mistral Large and NeMo - 80% price drop
- Low latency - faster response compared to Mistral's LLMs
- Flexible - can be deployed across different environments with less restrictions on required resources. </p>
<p>Mistral Small is great for: 
- Text based tasks such as summarization, sentiment analysis and translation. 
- Applications where frequent requests are made due to its cost effectiveness 
- Low latency code tasks like review and code suggestions </p>
<h2 id="comparing-mistral-small-and-mistral-large">Comparing Mistral Small and Mistral Large</h2>
<p>To show differences in latency between Mistral Small and Large, run the below cells. </p>
<p>You should see a difference in response times between 3-5 seconds. Also note the response lengths and style over the same prompt.  </p>
<pre><code class="language-python">
import os 
endpoint = &quot;https://models.inference.ai.azure.com&quot;
model_name = &quot;Mistral-small&quot;
token = os.environ[&quot;GITHUB_TOKEN&quot;]

client = ChatCompletionsClient(
    endpoint=endpoint,
    credential=AzureKeyCredential(token),
)

response = client.complete(
    messages=[
        SystemMessage(content=&quot;You are a helpful coding assistant.&quot;),
        UserMessage(content=&quot;Can you write a Python function to the fizz buzz test?&quot;),
    ],
    temperature=1.0,
    top_p=1.0,
    max_tokens=1000,
    model=model_name
)

print(response.choices[0].message.content)

</code></pre>
<pre><code class="language-python">
import os
from azure.ai.inference import ChatCompletionsClient
from azure.ai.inference.models import SystemMessage, UserMessage
from azure.core.credentials import AzureKeyCredential

endpoint = &quot;https://models.inference.ai.azure.com&quot;
model_name = &quot;Mistral-large&quot;
token = os.environ[&quot;GITHUB_TOKEN&quot;]

client = ChatCompletionsClient(
    endpoint=endpoint,
    credential=AzureKeyCredential(token),
)

response = client.complete(
    messages=[
        SystemMessage(content=&quot;You are a helpful coding assistant.&quot;),
        UserMessage(content=&quot;Can you write a Python function to the fizz buzz test?&quot;),
    ],
    temperature=1.0,
    top_p=1.0,
    max_tokens=1000,
    model=model_name
)

print(response.choices[0].message.content)

</code></pre>
<h2 id="mistral-nemo">Mistral NeMo</h2>
<p>Compared to the other two models discussed in this lesson, Mistral NeMo is the only free model with an Apache2 License. </p>
<p>It is viewed as an upgrade to the earlier open source LLM from Mistral, Mistral 7B. </p>
<p>Some other feature of the NeMo model are: </p>
<ul>
<li>
<p><em>More efficient tokenization:</em> This model using the Tekken tokenizer over the more commonly used tiktoken. This allows for better performance over more languages and code. </p>
</li>
<li>
<p><em>Finetuning:</em> The base model is available for finetuning. This allows for more flexibility for use-cases where finetuning may be needed. </p>
</li>
<li>
<p><em>Native Function Calling</em> - Like Mistral Large, this model has been trained on function calling. This makes it unique as being one of the first open source models to do so. </p>
</li>
</ul>
<h3 id="comparing-tokenizers">Comparing Tokenizers</h3>
<p>In this sample, we will look at how Mistral NeMo handles tokenization compared to Mistral Large. </p>
<p>Both samples take the same prompt but you should see that NeMo returns back less tokens vs Mistral Large. </p>
<pre><code class="language-bash">pip install mistral-common
</code></pre>
<pre><code class="language-python"># Import needed packages:
from mistral_common.protocol.instruct.messages import (
    UserMessage,
)
from mistral_common.protocol.instruct.request import ChatCompletionRequest
from mistral_common.protocol.instruct.tool_calls import (
    Function,
    Tool,
)
from mistral_common.tokens.tokenizers.mistral import MistralTokenizer

# Load Mistral tokenizer

model_name = &quot;open-mistral-nemo &quot;

tokenizer = MistralTokenizer.from_model(model_name)

# Tokenize a list of messages
tokenized = tokenizer.encode_chat_completion(
    ChatCompletionRequest(
        tools=[
            Tool(
                function=Function(
                    name=&quot;get_current_weather&quot;,
                    description=&quot;Get the current weather&quot;,
                    parameters={
                        &quot;type&quot;: &quot;object&quot;,
                        &quot;properties&quot;: {
                            &quot;location&quot;: {
                                &quot;type&quot;: &quot;string&quot;,
                                &quot;description&quot;: &quot;The city and state, e.g. San Francisco, CA&quot;,
                            },
                            &quot;format&quot;: {
                                &quot;type&quot;: &quot;string&quot;,
                                &quot;enum&quot;: [&quot;celsius&quot;, &quot;fahrenheit&quot;],
                                &quot;description&quot;: &quot;The temperature unit to use. Infer this from the users location.&quot;,
                            },
                        },
                        &quot;required&quot;: [&quot;location&quot;, &quot;format&quot;],
                    },
                )
            )
        ],
        messages=[
            UserMessage(content=&quot;What's the weather like today in Paris&quot;),
        ],
        model=model_name,
    )
)
tokens, text = tokenized.tokens, tokenized.text

# Count the number of tokens
print(len(tokens))
</code></pre>
<pre><code class="language-python"># Import needed packages:
from mistral_common.protocol.instruct.messages import (
    UserMessage,
)
from mistral_common.protocol.instruct.request import ChatCompletionRequest
from mistral_common.protocol.instruct.tool_calls import (
    Function,
    Tool,
)
from mistral_common.tokens.tokenizers.mistral import MistralTokenizer

# Load Mistral tokenizer

model_name = &quot;mistral-large-latest&quot;

tokenizer = MistralTokenizer.from_model(model_name)

# Tokenize a list of messages
tokenized = tokenizer.encode_chat_completion(
    ChatCompletionRequest(
        tools=[
            Tool(
                function=Function(
                    name=&quot;get_current_weather&quot;,
                    description=&quot;Get the current weather&quot;,
                    parameters={
                        &quot;type&quot;: &quot;object&quot;,
                        &quot;properties&quot;: {
                            &quot;location&quot;: {
                                &quot;type&quot;: &quot;string&quot;,
                                &quot;description&quot;: &quot;The city and state, e.g. San Francisco, CA&quot;,
                            },
                            &quot;format&quot;: {
                                &quot;type&quot;: &quot;string&quot;,
                                &quot;enum&quot;: [&quot;celsius&quot;, &quot;fahrenheit&quot;],
                                &quot;description&quot;: &quot;The temperature unit to use. Infer this from the users location.&quot;,
                            },
                        },
                        &quot;required&quot;: [&quot;location&quot;, &quot;format&quot;],
                    },
                )
            )
        ],
        messages=[
            UserMessage(content=&quot;What's the weather like today in Paris&quot;),
        ],
        model=model_name,
    )
)
tokens, text = tokenized.tokens, tokenized.text

# Count the number of tokens
print(len(tokens))
</code></pre>
<h2 id="learning-does-not-stop-here-continue-the-journey">Learning does not stop here, continue the Journey</h2>
<p>After completing this lesson, check out our <a href="https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst">Generative AI Learning collection</a> to continue leveling up your Generative AI knowledge!</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.83f73b43.min.js"></script>
      
    
  </body>
</html>